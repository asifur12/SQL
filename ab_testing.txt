-- 1.	Compare the final_assignments_qa table to the assignment events we captured for user_level_testing.--Write an answer to the following question: Does this table have everything you need to compute metrics --like 30-day view-binary?-- No, date column needed.--2 2.	Write a query and table creation statement to make --final_assignments_qa look like the final_assignments table.--If you discovered something missing in part 1, you may fill in the value with a place holder of the --appropriate data type. SELECT   item_id,  test_a as test_assignment,  (CASE WHEN test_a IS NOT NULL THEN 'test_a'   ELSE NULL END) as test_number,  (CASE WHEN test_a IS NOT NULL THEN '2015-03-14 00:00:00'   ELSE NULL END) as test_start_dateFROM   dsv1069.final_assignments_qaUNION SELECT   item_id,  test_a as test_assignment,  (CASE WHEN test_b IS NOT NULL THEN 'test_b'   ELSE NULL END) as test_number,  (CASE WHEN test_b IS NOT NULL THEN '2015-03-14 00:00:00'   ELSE NULL END) as test_start_dateFROM   dsv1069.final_assignments_qa--3.	Use the final_assignments table to calculate the order binary for the 30 day--window after the test assignment for item_test_2 (You may include the day the test started)SELECT   fa.item_id,  fa.test_start_date,  max(CASE       WHEN o.paid_at BETWEEN fa.test_start_date and fa.test_start_date + INTERVAL '30 days' THEN 1      ELSE 0 END) as order_binary      FROM   dsv1069.final_assignments fa LEFT JOIN   dsv1069.orders o ON fa.item_id  = o.item_idWHERE   fa.test_number = 'item_test_2'GROUP BY   fa.item_id, fa.test_start_date-- 4 Use the final_assignments table to calculate the view binary, and-- average views for the 30 day window after the test assignment for-- item_test_2. (You may include the day the test started)SELECT   fa.item_id,  fa.test_assignment,  fa.test_number,   max(CASE       WHEN events.event_time BETWEEN fa.test_start_date and fa.test_start_date + INTERVAL '30 days' THEN 1      ELSE 0 END) as view_binary      FROM dsv1069.final_assignments faLEFT JOIN(SELECTCASE WHEN parameter_name = 'item_id' THEN CAST( parameter_value as NUMERIC) ELSE NULL END as item_id,event_timeFROM dsv1069.events eWHERE event_name = 'view_item') AS eventsON fa.item_id = events.item_idWHERE fa.test_number = 'item_test_2'GROUP BY   fa.test_start_date, fa.item_id, events.item_id, fa.test_assignment, fa.test_number--5 Use the https://thumbtack.github.io/abba/demo/abba.htmlto compute the lifts in metrics and the p-values --for the binary metrics ( 30 day order binary and 30 day view binary) using a interval 95% confidence. SELECT test_assignment,       test_number,       COUNT(DISTINCT item) AS number_of_items,       SUM(view_binary_30d) AS view_binary_30dFROM  (SELECT final_assignments.item_id AS item,          test_assignment,          test_number,          test_start_date,          MAX((CASE                   WHEN date(event_time) - date(test_start_date) BETWEEN 0 AND 30 THEN 1                   ELSE 0               END)) AS view_binary_30d   FROM dsv1069.final_assignments   LEFT JOIN dsv1069.view_item_events     ON final_assignments.item_id = view_item_events.item_id   WHERE test_number = 'item_test_2'1. Use Mode’s Report builder feature to write up the test. Your write-up should include a title, a graph for each of the two binary metrics you’ve calculated. The lift and p-value (from the AB test calculator) for each of the two metrics, and a complete sentence to interpret the significance of each of the results.* Successes and Total Trials: The Baseline had 1,130 successes out of 924 trials, and Variation 1 had 1,068 successes out of 894 trials.* Success Rates: These are erroneously reported as 122% for the Baseline and 119% for Variation 1, which suggests there might be a data entry error since the number of successes cannot exceed the number of trials.* Improvement: The computed improvement of -2.3% indicates that Variation 1 performed slightly worse than the Baseline.* p-value: The p-value is not available (NaN), which typically indicates an issue with the calculation process—possibly due to the erroneous input data.Final Interpretation Sentence:"Due to an apparent data inconsistency, with success rates exceeding 100%, this A/B test result is inconclusive and needs a review of data inputs to ensure accuracy and reliability of findings."It's important to resolve the discrepancies in the data input (i.e., the number of successes exceeding the number of trials) before drawing any valid conclusions from the test. Once corrected, the test should be re-run to obtain meaningful results and insights.